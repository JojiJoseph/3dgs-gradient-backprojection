import time
from typing import Literal
from natsort import natsorted
import tyro
import os
import torch
import cv2
import imageio # To generate gifs
import pycolmap_scene_manager as pycolmap
from gsplat import rasterization
from ultralytics import YOLOWorld
from sam2.build_sam import build_sam2_video_predictor
import numpy as np

from dataclasses import dataclass
from pathlib import Path
from typing import Union

import clip
import cv2
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('TkAgg')
import torch
import tyro
from sklearn.decomposition import PCA

from tqdm import tqdm

from lseg import LSegNet

import pickle as pkl

import imageio
import pycolmap_scene_manager as pycolmap
import torch
import os
import faiss
import base64
import cv2
import json

def torch_to_cv(tensor):
    img_cv = tensor.detach().cpu().numpy()[..., ::-1]
    img_cv = np.clip(img_cv*255, 0, 255).astype(np.uint8)
    return img_cv


def _detach_tensors_from_dict(d, inplace=True):
    if not inplace:
        d = d.copy()
    for key in d:
        if isinstance(d[key], torch.Tensor):
            d[key] = d[key].detach()
    return d


def load_checkpoint(checkpoint: str, data_dir: str, rasterizer: Literal["original", "gsplat"]="original", data_factor: int = 1):

    colmap_project = pycolmap.SceneManager(f"{data_dir}/sparse/0")
    colmap_project.load_cameras()
    colmap_project.load_images()
    colmap_project.load_points3D()
    model = torch.load(checkpoint) # Make sure it is generated by 3DGS original repo
    if rasterizer == "original":
        model_params, _ = model
        splats = {
            "active_sh_degree": model_params[0],
            "means": model_params[1],
            "features_dc": model_params[2],
            "features_rest": model_params[3],
            "scaling": model_params[4],
            "rotation": model_params[5],
            "opacity": model_params[6].squeeze(1),
        }
    elif rasterizer == "gsplat":
        print(model["splats"].keys())
        model_params = model["splats"]
        splats = {
            "active_sh_degree": 3,
            "means": model_params["means"],
            "features_dc": model_params["sh0"],
            "features_rest": model_params["shN"],
            "scaling": model_params["scales"],
            "rotation": model_params["quats"],
            "opacity": model_params["opacities"],
        }
    else:
        raise ValueError("Invalid rasterizer")

    _detach_tensors_from_dict(splats)

    # Assuming only one camera
    for camera in colmap_project.cameras.values():
        camera_matrix = torch.tensor(
            [
                [camera.fx, 0, camera.cx],
                [0, camera.fy, camera.cy],
                [0, 0, 1],
            ]
        )
        break

    camera_matrix[:2,:3] /= data_factor

    splats["camera_matrix"] = camera_matrix
    splats["colmap_project"] = colmap_project
    splats["colmap_dir"] = data_dir

    return splats

def get_viewmat_from_colmap_image(image):
    viewmat = torch.eye(4).float()#.to(device)
    viewmat[:3, :3] = torch.tensor(image.R()).float()#.to(device)
    viewmat[:3, 3] = torch.tensor(image.t).float()#.to(device)
    return viewmat

def create_checkerboard(width, height, size=64):
    checkerboard = np.zeros((height, width, 3), dtype=np.uint8)
    for y in range(0, height, size):
        for x in range(0, width, size):
            if (x // size + y // size) % 2 == 0:
                checkerboard[y:y + size, x:x + size] = 255
            else:
                checkerboard[y:y + size, x:x + size] = 128
    return checkerboard




def render_to_dir(output_dir: str, splats, feedback: bool = False):
    if feedback:
        cv2.destroyAllWindows()
        cv2.namedWindow("Initial Rendering", cv2.WINDOW_NORMAL)
    os.makedirs(output_dir, exist_ok=True)
    colmap_project = splats["colmap_project"]
    frame_idx = 0
    for image in sorted(colmap_project.images.values(), key=lambda x: x.name):
        image_name = image.name#.split(".")[0] + ".jpg"
        image_path = f"{output_dir}/{image_name}"
        means = splats["means"]
        colors_dc = splats["features_dc"]
        colors_rest = splats["features_rest"]
        colors = torch.cat([colors_dc, colors_rest], dim=1)
        opacities = torch.sigmoid(splats["opacity"])
        scales = torch.exp(splats["scaling"])
        quats = splats["rotation"]
        viewmat = get_viewmat_from_colmap_image(image)
        K = splats["camera_matrix"]
        output, _, info = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors,
            viewmats=viewmat[None],
            Ks=K[None],
            sh_degree=3,
            width=K[0, 2] * 2,
            height=K[1, 2] * 2,
        )
        output_cv = torch_to_cv(output[0])
        imageio.imsave(image_path, output_cv[:, :, ::-1])
        if feedback:
            cv2.imshow("Initial Rendering", output_cv)
            cv2.waitKey(1)
        frame_idx += 1

def prune_by_gradients(splats):
    colmap_project = splats["colmap_project"]
    frame_idx = 0
    means = splats["means"]
    colors_dc = splats["features_dc"]
    colors_rest = splats["features_rest"]
    colors = torch.cat([colors_dc, colors_rest], dim=1)
    opacities = torch.sigmoid(splats["opacity"])
    scales = torch.exp(splats["scaling"])
    quats = splats["rotation"]
    K = splats["camera_matrix"]
    colors.requires_grad = True
    gaussian_grads = torch.zeros(colors.shape[0], device=colors.device)
    for image in sorted(colmap_project.images.values(), key=lambda x: x.name):
        viewmat = get_viewmat_from_colmap_image(image)
        output, _, _ = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors[:,0,:],
            viewmats=viewmat[None],
            Ks=K[None],
            # sh_degree=3,
            width=K[0, 2] * 2,
            height=K[1, 2] * 2,
        )
        frame_idx += 1
        pseudo_loss = ((output.detach() + 1 - output)**2).mean()
        pseudo_loss.backward()
        # print(colors.grad.shape)
        gaussian_grads += (colors.grad[:,0]).norm(dim=[1])
        colors.grad.zero_()

    mask = gaussian_grads > 0
    print("Total splats", len(gaussian_grads))
    print("Pruned", (~mask).sum(), "splats")
    print("Remaining", mask.sum(), "splats")
    splats = splats.copy()
    splats["means"] = splats["means"][mask]
    splats["features_dc"] = splats["features_dc"][mask]
    splats["features_rest"] = splats["features_rest"][mask]
    splats["scaling"] = splats["scaling"][mask]
    splats["rotation"] = splats["rotation"][mask]
    splats["opacity"] = splats["opacity"][mask]
    return splats

def test_proper_pruning(splats, splats_after_pruning):
    colmap_project = splats["colmap_project"]
    frame_idx = 0
    means = splats["means"]
    colors_dc = splats["features_dc"]
    colors_rest = splats["features_rest"]
    colors = torch.cat([colors_dc, colors_rest], dim=1)
    opacities = torch.sigmoid(splats["opacity"])
    scales = torch.exp(splats["scaling"])
    quats = splats["rotation"]

    means_pruned = splats_after_pruning["means"]
    colors_dc_pruned = splats_after_pruning["features_dc"]
    colors_rest_pruned = splats_after_pruning["features_rest"]
    colors_pruned = torch.cat([colors_dc_pruned, colors_rest_pruned], dim=1)
    opacities_pruned = torch.sigmoid(splats_after_pruning["opacity"])
    scales_pruned = torch.exp(splats_after_pruning["scaling"])
    quats_pruned = splats_after_pruning["rotation"]



    K = splats["camera_matrix"]
    total_error = 0
    max_pixel_error = 0
    for image in sorted(colmap_project.images.values(), key=lambda x: x.name):
        viewmat = get_viewmat_from_colmap_image(image)
        output, _, _ = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors,
            viewmats=viewmat[None],
            Ks=K[None],
            sh_degree=3,
            width=K[0, 2] * 2,
            height=K[1, 2] * 2,
        )

        output_pruned, _, _ = rasterization(
            means_pruned,
            quats_pruned,
            scales_pruned,
            opacities_pruned,
            colors_pruned,
            viewmats=viewmat[None],
            Ks=K[None],
            sh_degree=3,
            width=K[0, 2] * 2,
            height=K[1, 2] * 2,
        )

        total_error += torch.abs((output - output_pruned)).sum()
        max_pixel_error = max(max_pixel_error, torch.abs((output - output_pruned)).max())

    percentage_pruned = (len(splats["means"]) - len(splats_after_pruning["means"])) / len(splats["means"]) * 100

    assert max_pixel_error < 1 / (255*2), "Max pixel error should be less than 1/(255*2), safety margin"
    print("Report {}% pruned, max pixel error = {}, total pixel error = {}".format(percentage_pruned, max_pixel_error, total_error))


def get_mask3d(splats, prompt, data_dir: str, results_dir: str, show_visual_feedback: bool = False, mask_interval: int = 1, voting_method: Literal["gradient", "binary", "projection"] = "gradient", mask_dir=None):
    checkpoint = "./checkpoints/sam2_hiera_large.pt"
    if not os.path.exists(checkpoint):
        raise RuntimeError("Please download the checkpoint sam2_hiera_large.pt to checkpoints folder")
    
    if show_visual_feedback:
        cv2.destroyAllWindows()
        cv2.namedWindow("2D Mask", cv2.WINDOW_NORMAL)
    model_cfg = "sam2_hiera_l.yaml"
    mask_predictor = build_sam2_video_predictor(model_cfg, checkpoint)
    yolo_world = YOLOWorld("yolov8s-worldv2.pt")
    yolo_world.set_classes([prompt])
    colmap_project = splats["colmap_project"]
    first_image_name = sorted(colmap_project.images.values(), key=lambda x: x.name)[0].name
    first_image_path = f"{results_dir}/images/{first_image_name}"
    frame_idx = 0
    with torch.autocast("cuda", dtype=torch.bfloat16):
        state = mask_predictor.init_state(f"{results_dir}/images/")

        result = yolo_world(first_image_path)[0]

        box = result.boxes[0].xyxy[0].tolist()


        # add new prompts and instantly get the output on the same frame
        _, object_ids, masks = mask_predictor.add_new_points_or_box(
            state, box=box, frame_idx=0, obj_id=0
        )

        means = splats["means"]
        colors_dc = splats["features_dc"]
        colors_rest = splats["features_rest"]

        colors = colors_dc[:,0,:] * 0


        opacities = torch.sigmoid(splats["opacity"])
        scales = torch.exp(splats["scaling"])
        quats = splats["rotation"]
        K = splats["camera_matrix"]
        colors.requires_grad = True

        gaussian_grads = torch.zeros(colors.shape[0], device=colors.device)
        mask_dir = f"{results_dir}/masks_with_images"
        mask_bin_dir = f"{results_dir}/masks_bin"
        os.makedirs(mask_dir, exist_ok=True)
        os.makedirs(mask_bin_dir, exist_ok=True)
        
        # propagate the prompts to get masklets throughout the video
        frame_idx = 0
        for image, (frame_idx, object_ids, masks) in zip(
            sorted(colmap_project.images.values(), key=lambda x: x.name),
            mask_predictor.propagate_in_video(state),
        ):

            image_name = image.name#.split(".")[0] + ".jpg"
            # image_name = f"frame_"
            image_path = f"{results_dir}/images/{image_name}"
            mask_path = f"{mask_dir}/{image_name}"
            mask_bin_path = f"{mask_bin_dir}/{image.name}"

            frame = cv2.imread(image_path)
            mask = masks[0, 0].cpu().numpy() >= 0
            mask = mask.astype(float)
            mask = cv2.blur(mask, (7, 7))
            mask = mask > 0
            mask = mask.astype(bool)

            mask_red = np.zeros_like(frame)

            mask_red[:, :, -1][mask] = 255
            mask_bin = mask.astype(np.uint8) * 255
            cv2.imwrite(mask_bin_path, mask_bin)
            output = cv2.addWeighted(frame, 1, mask_red, 0.5, 0)
            cv2.imwrite(mask_path, output)


            frame_idx += 1
            if (frame_idx % mask_interval != 1) and (mask_interval != 1):
                continue
            if show_visual_feedback:
                cv2.imshow("2D Mask", output)
                cv2.waitKey(1)
            
            viewmat = get_viewmat_from_colmap_image(image)

            
            width = frame.shape[1]
            height = frame.shape[0]

            output_for_grad, _, meta = rasterization(
                means,
                quats,
                scales,
                opacities,
                colors,
                viewmat[None],
                K[None],
                width=width,
                height=height,
                # sh_degree=3,
            )

            target = output_for_grad[0] * torch.from_numpy(mask)[..., None].cuda().float()
            loss = 1 * target.mean()

            loss.backward(retain_graph=True)
            if voting_method == "gradient":
                mins = torch.min(colors.grad, dim=-1).values
                maxes = torch.max(colors.grad, dim=-1).values
                assert torch.allclose(mins , maxes), "Something is wrong with gradient calculation"
                gaussian_grads += (colors.grad).norm(dim=[1])
            elif voting_method == "binary":
                gaussian_grads += 1 * (colors.grad.norm(dim=[1]) > 0)
            elif voting_method == "projection":

                means2d = np.round(meta["means2d"].detach().cpu().numpy()).astype(int)
                means2d_mask = (means2d[:, 0] >= 0) & (means2d[:, 0] < width) & (means2d[:, 1] >= 0) & (means2d[:, 1] < height)
                means2d = means2d[means2d_mask]
                gaussian_ids = meta["gaussian_ids"].detach().cpu().numpy()
                gaussian_ids = gaussian_ids[means2d_mask]

                means2d_mask = mask[means2d[:, 1], means2d[:, 0]] # Check if the splat is in the mask
                gaussian_grads[torch.from_numpy(gaussian_ids[~means2d_mask]).long()] -= 1
                gaussian_grads[torch.from_numpy(gaussian_ids[means2d_mask]).long()] += 1
            else:
                raise ValueError("Invalid voting method")

            colors.grad.zero_()


            mask_inverted = ~mask
            target = output_for_grad[0] * torch.from_numpy(mask_inverted).cuda()[
                ..., None
            ]
            loss = 1 * target.mean()
            loss.backward(retain_graph=False)

            if voting_method == "gradient":
                gaussian_grads -= (colors.grad).norm(dim=[1])
            elif voting_method == "binary":
                gaussian_grads -= 1 * ((colors.grad).norm(dim=[1]) > 0)
            elif voting_method == "projection":
                pass
            else:
                raise ValueError("Invalid voting method")
            colors.grad.zero_()

        mask_3d = gaussian_grads > 0
        mask_3d_inverted = gaussian_grads < 0 # We don't need Gaussians without any influence ie gaussian_grads == 0
        return mask_3d, mask_3d_inverted

def apply_mask3d(splats, mask3d,mask3d_inverted, results_dir: str):
    if mask3d_inverted == None:
        mask3d_inverted = ~mask3d
    extracted = splats.copy()
    deleted = splats.copy()
    masked = splats.copy()
    extracted["means"] = extracted["means"][mask3d]
    extracted["features_dc"] = extracted["features_dc"][mask3d]
    extracted["features_rest"] = extracted["features_rest"][mask3d]
    extracted["scaling"] = extracted["scaling"][mask3d]
    extracted["rotation"] = extracted["rotation"][mask3d]
    extracted["opacity"] = extracted["opacity"][mask3d]

    deleted["means"] = deleted["means"][mask3d_inverted]
    deleted["features_dc"] = deleted["features_dc"][mask3d_inverted]
    deleted["features_rest"] = deleted["features_rest"][mask3d_inverted]
    deleted["scaling"] = deleted["scaling"][mask3d_inverted]
    deleted["rotation"] = deleted["rotation"][mask3d_inverted]
    deleted["opacity"] = deleted["opacity"][mask3d_inverted]

    masked["features_dc"][mask3d] = 1#(1 - 0.5) / 0.2820947917738781
    masked["features_dc"][~mask3d] = 0#(0 - 0.5) / 0.2820947917738781
    masked["features_rest"][~mask3d] = 0

    return extracted, deleted, masked

    

def render_to_gif(output_path: str, splats, feedback: bool = False, use_checkerboard_background: bool = False, no_sh: bool=False):
    if feedback:
        cv2.destroyAllWindows()
        cv2.namedWindow("Rendering", cv2.WINDOW_NORMAL)
    frames = []
    means = splats["means"]
    colors_dc = splats["features_dc"]
    colors_rest = splats["features_rest"]
    colors = torch.cat([colors_dc, colors_rest], dim=1)
    if no_sh == True:
        colors = colors_dc[:,0,:]
    opacities = torch.sigmoid(splats["opacity"])
    scales = torch.exp(splats["scaling"])
    quats = splats["rotation"]
    K = splats["camera_matrix"]
    aux_dir = output_path + ".images"
    os.makedirs(aux_dir, exist_ok=True)
    for image in sorted(splats["colmap_project"].images.values(), key=lambda x: x.name):
        viewmat = get_viewmat_from_colmap_image(image)
        output, alphas, meta = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors,
            viewmat[None],
            K[None],
            width=K[0, 2]*2,
            height=K[1, 2]*2,
            sh_degree=3 if not no_sh else None,
        )
        frame = np.clip(output[0].detach().cpu().numpy() * 255, 0, 255).astype(np.uint8)
        if use_checkerboard_background:
            checkerboard = create_checkerboard(frame.shape[1], frame.shape[0])
            alphas = alphas[0].detach().cpu().numpy()
            frame = frame * alphas + checkerboard * (1 - alphas)
            frame = np.clip(frame, 0, 255).astype(np.uint8)
        frames.append(frame)
        if feedback:
            cv2.imshow("Rendering", frame[...,::-1])
            cv2.imwrite(f"{aux_dir}/{image.name}", frame[...,::-1])
            cv2.waitKey(1)
    imageio.mimsave(output_path, frames, fps=10, loop=0)
    if feedback:
        cv2.destroyAllWindows()

def render_mask_pred(output_dir: str, splats, feedback: bool = False):
    if feedback:
        cv2.destroyAllWindows()
        cv2.namedWindow("Rendering", cv2.WINDOW_NORMAL)
    frames = []
    means = splats["means"]
    colors_dc = splats["features_dc"]
    colors_rest = splats["features_rest"]

    colors = colors_dc[:,0,:]
    opacities = torch.sigmoid(splats["opacity"])
    scales = torch.exp(splats["scaling"])
    quats = splats["rotation"]
    K = splats["camera_matrix"]
    aux_dir = output_dir
    os.makedirs(aux_dir, exist_ok=True)
    for image in sorted(splats["colmap_project"].images.values(), key=lambda x: x.name):
        viewmat = get_viewmat_from_colmap_image(image)
        output, alphas, meta = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors,
            viewmat[None],
            K[None],
            width=K[0, 2]*2,
            height=K[1, 2]*2,
            sh_degree=None,
        )
        frame = np.clip(output[0].detach().cpu().numpy() * 255, 0, 255).astype(np.uint8)
        frame = frame > 128
        frame = frame.astype(np.uint8) * 255
            # cv2.imshow("Checkerboard", checkerboard)
        # frames.append(frame)
        if feedback:
            cv2.imshow("Rendering", frame[...,::-1])
            cv2.imwrite(f"{aux_dir}/{image.name}", frame)
            cv2.waitKey(1)
    # imageio.mimsave(output_path, frames, fps=10)
    if feedback:
        cv2.destroyAllWindows()

def create_feature_field(splats, results_dir: str, show_visual_feedback: bool = False, mask_interval: int = 1, voting_method: Literal["gradient", "binary", "projection"] = "gradient", mask_dir=None):
    

    frame_idx = 0

    means = splats["means"]
    colors_dc = splats["features_dc"]
    colors_rest = splats["features_rest"]

    colors = colors_dc[:,0,:] #* 0
    colors_0 = colors_dc[:,0,:] * 0
    colmap_project = splats["colmap_project"]


    opacities = torch.sigmoid(splats["opacity"])
    scales = torch.exp(splats["scaling"])
    quats = splats["rotation"]
    K = splats["camera_matrix"]
    colors.requires_grad = True
    colors_0.requires_grad = True

    gaussian_features = torch.zeros(colors.shape[0],3, device=colors.device)
    gaussian_denoms = torch.zeros(colors.shape[0], device=colors.device)
    mask_dir = f"{results_dir}/masks_with_images"
    mask_bin_dir = f"{results_dir}/masks_bin"
    os.makedirs(mask_dir, exist_ok=True)
    os.makedirs(mask_bin_dir, exist_ok=True)
    
    # propagate the prompts to get masklets throughout the video
    frame_idx = 0
    for image in sorted(colmap_project.images.values(), key=lambda x: x.name):

        image_name = image.name#.split(".")[0] + ".jpg"
        
        viewmat = get_viewmat_from_colmap_image(image)

        
        width = int(K[0, 2] * 2)
        height = int(K[1, 2] * 2)

        output_for_grad, _, meta = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors,
            viewmat[None],
            K[None],
            width=width,
            height=height,
            # sh_degree=3,
        )

        output_img = torch_to_cv(output_for_grad[0])
        output_gray = cv2.cvtColor(output_img, cv2.COLOR_BGR2GRAY)
        # combine channels
        output_gray_3 = np.stack([output_gray, output_gray, output_gray], axis=-1)
        output_gray_3_torch = torch.from_numpy(output_gray_3).float().cuda() / 255
        # output_gray_3_torch = output_for_grad[...,[0,1,2]].detach().clone()[0]

        # print(output_gray_3_torch.shape, output_gray_3_torch.min(), output_gray_3_torch.max())  

        

        target = (output_for_grad[0] * output_gray_3_torch).mean()

        target.backward()

        gaussian_features += colors.grad

        output_for_grad, _, meta = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors_0,
            viewmat[None],
            K[None],
            width=width,
            height=height,
            # sh_degree=3,
        )

        target_0 = (output_for_grad[0]).mean()

        target_0.backward()
        
        gaussian_denoms += colors_0.grad[:,0]

    gaussian_features = gaussian_features / gaussian_denoms[:, None]
    for image in sorted(colmap_project.images.values(), key=lambda x: x.name):
        image_name = image.name#.split(".")[0] + ".jpg"
        
        viewmat = get_viewmat_from_colmap_image(image)

        
        width = int(K[0, 2] * 2)
        height = int(K[1, 2] * 2)

        output_for_grad, _, meta = rasterization(
            means,
            quats,
            scales,
            opacities,
            gaussian_features,
            viewmat[None],
            K[None],
            width=width,
            height=height,
            # sh_degree=3,
        )

        print(gaussian_features.shape, gaussian_features.min(), gaussian_features.max())

        output_img = torch_to_cv(output_for_grad[0])
        cv2.imshow("Feature Field", output_img)
        cv2.waitKey(1)

def create_feature_field_dino_old(splats, results_dir: str, show_visual_feedback: bool = False, mask_interval: int = 1, voting_method: Literal["gradient", "binary", "projection"] = "gradient", mask_dir=None):
    
    dinov2_vits14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')
    dinov2_vits14_reg.eval()

    dinov2_vits14_reg.cuda()
    # print(dinov2_vits14_reg.__dir__())
    # exit()

    frame_idx = 0

    mask_3d = torch.load(f"mask3d.pth").cuda()
    print(splats["means"].shape, mask_3d.shape)
    means = splats["means"][mask_3d]
    colors_dc = splats["features_dc"][mask_3d]
    colors_rest = splats["features_rest"][mask_3d]
    colors_all = torch.cat([colors_dc, colors_rest], dim=1)

    colors = colors_dc[:,0,:] #* 0
    colors_0 = colors_dc[:,0,:] * 0
    colmap_project = splats["colmap_project"]


    opacities = torch.sigmoid(splats["opacity"])[mask_3d]
    scales = torch.exp(splats["scaling"])[mask_3d]
    quats = splats["rotation"][mask_3d]
    K = splats["camera_matrix"]
    colors.requires_grad = True
    colors_0.requires_grad = True

    gaussian_features = torch.zeros(colors.shape[0],DIM, device=colors.device)
    gaussian_denoms = torch.zeros(colors.shape[0], device=colors.device)
    mask_dir = f"{results_dir}/masks_with_images"
    mask_bin_dir = f"{results_dir}/masks_bin"
    os.makedirs(mask_dir, exist_ok=True)
    os.makedirs(mask_bin_dir, exist_ok=True)
    t1 = time.time()
    # propagate the prompts to get masklets throughout the video
    frame_idx = 0
    colors_feats = torch.zeros(colors.shape[0],DIM, device=colors.device)
    colors_feats.requires_grad = True
    colors_feats_0 = torch.zeros(colors.shape[0],3, device=colors.device)
    colors_feats_0.requires_grad = True
    for image in sorted(colmap_project.images.values(), key=lambda x: x.name):

        image_name = image.name#.split(".")[0] + ".jpg"
        
        viewmat = get_viewmat_from_colmap_image(image)

        
        width = int(K[0, 2] * 2)
        height = int(K[1, 2] * 2)
        with torch.no_grad():
            output, _, meta = rasterization(
                means,
                quats,
                scales,
                opacities,
                colors_all,
                viewmat[None],
                K[None],
                width=width,
                height=height,
                sh_degree=3,
            )
            # Resize output to 224x224
            output = torch.nn.functional.interpolate(output.permute(0,3,1,2).cuda(), size=(224, 224), mode='bilinear', align_corners=False)
            feats = dinov2_vits14_reg.forward_features(output)["x_norm_patchtokens"]
            feats = feats[0].reshape((16, 16, DIM))
            # print(feats.shape)
            # Resize to width x height
            feats = torch.nn.functional.interpolate(feats.unsqueeze(0).permute(0,3,1,2), size=(height, width), mode='nearest', align_corners=False)[0]
            # print(feats.shape)
            feats = feats.permute(1,2,0)
            # feats = feats / feats.norm(dim=-1, keepdim=True)

        # colors_feats = torch.zeros(colors.shape[0],3, device=colors.device)

        output_for_grad, _, meta = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors_feats,
            viewmat[None],
            K[None],
            width=width,
            height=height,
            # sh_degree=3,
        )

        # output_img = torch_to_cv(output_for_grad[0])

        
        # output_gray = cv2.cvtColor(output_img, cv2.COLOR_BGR2GRAY)
        # # combine channels
        # output_gray_3 = np.stack([output_gray, output_gray, output_gray], axis=-1)
        # output_gray_3_torch = torch.from_numpy(output_gray_3).float().cuda() / 255
        # output_gray_3_torch = output_for_grad[...,[0,1,2]].detach().clone()[0]

        # print(output_gray_3_torch.shape, output_gray_3_torch.min(), output_gray_3_torch.max())  

        
        # print(output_for_grad[0].shape, feats.shape)

        target = (output_for_grad[0] * feats).mean()

        target.backward()

        colors_feats_copy = colors_feats.grad.clone()

        # gaussian_features += colors_feats.grad

        colors_feats.grad.zero_()

        output_for_grad, _, meta = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors_feats_0,
            viewmat[None],
            K[None],
            width=width,
            height=height,
            # sh_degree=3,
        )

        target_0 = (output_for_grad[0]).mean()

        target_0.backward()
        
        # gaussian_denoms += colors_feats_0.grad[:,0]
        gaussian_features += colors_feats_copy / (colors_feats_0.grad[:,0:1]+1e-12)
        colors_feats_0.grad.zero_()

    # gaussian_features = gaussian_features / gaussian_denoms[:, None]
    gaussian_features = gaussian_features / gaussian_features.norm(dim=-1, keepdim=True)
    t2 = time.time()
    print("Time taken for feature distillation", t2-t1)
    from sklearn.decomposition import PCA
    pca = PCA(n_components=3)
    pca.fit(gaussian_features.cpu().numpy())
    cnt = 0
    for image in sorted(colmap_project.images.values(), key=lambda x: x.name):
        image_name = image.name#.split(".")[0] + ".jpg"
        
        viewmat = get_viewmat_from_colmap_image(image)

        
        width = int(K[0, 2] * 2)
        height = int(K[1, 2] * 2)

        output_for_grad, _, meta = rasterization(
            means,
            quats,
            scales,
            opacities,
            gaussian_features,
            viewmat[None],
            K[None],
            width=width,
            height=height,
            # sh_degree=3,
        )
        # output_for_grad = output_for_grad / output_for_grad.norm(dim=-1, keepdim=True)
        # print(output_for_grad.shape, output_for_grad.min(), output_for_grad.max())
        output = output_for_grad[0].detach().cpu().numpy()
        
        output_pca = pca.transform(output.reshape(-1, DIM)).reshape((height, width, 3))
        # print(output_pca.shape, output_pca.min(), output_pca.max())
        output_pca = output_pca - output_pca.min()
        output_pca = output_pca / output_pca.max()
        output_pca = (output_pca * 255).astype(np.uint8)
        # output_pca = output_pca.reshape((height, width, 3))
        cv2.imshow("Feature Field", output_pca)
        cv2.waitKey(1)
        # break
        if cnt == 90:
            cv2.imwrite("pca.png", output_pca)
            break
        cnt+=1
        

        # print(gaussian_features.shape, gaussian_features.min(), gaussian_features.max())

        # output_img = torch_to_cv(output_for_grad[0])
        # cv2.imshow("Feature Field", output_img)
        # cv2.waitKey(1)
    import faiss
    affordance_feats, labels = parse_affordance_example(affordance_example_pkl)
    # affordance_feats2, labels2 = parse_affordance_example(affordance_example_pkl_2)

    # affordance_feats = np.concatenate([affordance_feats, affordance_feats2], axis=0)
    # labels = np.concatenate([labels, labels2], axis=0)


    # Setting up faiss
    index_flat = faiss.IndexFlatIP(DIM)
    # index_flat = faiss.IndexFlatL2(DIM)
    index_flat.add(affordance_feats)
    t1 = time.time()
    D, I = index_flat.search(gaussian_features.detach().cpu().numpy(), 20)
    label_map = labels[I]

    def most_frequent(array):
        return np.bincount(array).argmax()
    # Applying the function along axis 1
    result = np.apply_along_axis(most_frequent, axis=1, arr=label_map)
    t2 = time.time()
    print("Time taken for query", t2-t1)
    print(result.shape, gaussian_features.shape)

    opacities_new = opacities.clone()
    colors_new = colors_all.clone()


    mask_3d = result.squeeze() == 1
    print(mask_3d.shape)
    mask_3d = torch.from_numpy(mask_3d).cuda()
    colors_new[mask_3d, 0, :] = torch.tensor([0.0, 0.0, 1.0]).cuda()
    # colors[mask_3d,1:,:] = 0
    opacities_new[mask_3d & (opacities_new > 0.5)] = 1.0

    mask_3d = result.squeeze() == 2
    mask_3d = torch.from_numpy(mask_3d).cuda()
    colors_new[mask_3d, 0, :] = torch.tensor([0.0, 1.0, 0.0]).cuda()
    # colors[mask_3d,1:,:] = 0
    opacities_new[mask_3d & (opacities_new > 0.5)] = 1.0

    mask_3d = result.squeeze() == 3
    mask_3d = torch.from_numpy(mask_3d).cuda()
    colors_new[mask_3d, 0, :] = torch.tensor([1.0, 0.0, 0.0]).cuda()
    # colors[mask_3d,1:,:] = 0
    opacities_new[mask_3d & (opacities_new > 0.5)] = 1.0

    mask_3d = result.squeeze() == 4
    mask_3d = torch.from_numpy(mask_3d).cuda()
    colors_new[mask_3d, 0, :] = torch.tensor([0.0, 1.0, 1.0]).cuda()
    # colors[mask_3d,1:,:] = 0
    opacities_new[mask_3d & (opacities_new > 0.5)] = 1.0
    cnt = 0
    for image in sorted(colmap_project.images.values(), key=lambda x: x.name):
        viewmat = get_viewmat_from_colmap_image(image)
        output, _, _ = rasterization(
            means,
            quats,
            scales,
            opacities_new,
            colors_new[:,0,:],
            viewmats=viewmat[None],
            Ks=K[None],
            # sh_degree=3,
            width=K[0, 2] * 2,
            height=K[1, 2] * 2,
            backgrounds=torch.ones((1, 3)).cuda()
        )
        output_cv = torch_to_cv(output[0])
        cv2.imshow("Output", output_cv)
        if cnt == 90:
            cv2.imwrite("affordance_transfer_new.png", output_cv)
            break
        cnt += 1

    for i in range( result.max()+1):
        mask = result == i
        new_scales = scales.clone()
        new_scales2 = scales.clone()
        new_scales[~mask] = 0
        new_scales2[mask] = 0
        for img in sorted(colmap_project.images.values(), key=lambda x: x.name):
            viewmat = get_viewmat_from_colmap_image(img)
            # print(mask.sum())
            output_for_grad, _, meta = rasterization(
                means,
                quats,
                new_scales,
                opacities,
                colors_all,
                viewmat[None],
                K[None],
                width=width,
                height=height,
                sh_degree=3,
                backgrounds=torch.ones((1, 3)).cuda()
            )
            output_img = torch_to_cv(output_for_grad[0])
            output_for_grad, _, meta = rasterization(
                means,
                quats,
                new_scales2,
                opacities,
                colors_all,
                viewmat[None],
                K[None],
                width=width,
                height=height,
                sh_degree=3,
                backgrounds=torch.ones((1, 3)).cuda()
            )
            output_img_2 = torch_to_cv(output_for_grad[0])
            output_img = np.concatenate([output_img, output_img_2], axis=1)
            cv2.imshow("Output", output_img)
            cv2.waitKey(100)
        # break


def create_feature_field_dino(splats, results_dir: str, show_visual_feedback: bool = False, mask_interval: int = 1, voting_method: Literal["gradient", "binary", "projection"] = "gradient", mask_dir=None):
    
    # net = LSegNet(
    #     backbone="clip_vitl16_384",
    #     features=256,
    #     crop_size=480,
    #     arch_option=0,
    #     block_depth=0,
    #     activation="lrelu",
    # )
    # # Load pre-trained weights
    # net.load_state_dict(torch.load("./checkpoints/lseg_minimal_e200.ckpt"))
    # net.eval()
    # net.cuda()

    # dinov2_vits14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14_reg')
    # dinov2_vits14_reg.eval()

    # dinov2_vits14_reg.cuda()

    dinov2_vits14_reg = feature_extractor

    # Preprocess the text prompt
    # clip_text_encoder = net.clip_pretrained.encode_text
    # prompt = clip.tokenize("chair")
    # prompt = prompt.cuda()






    means = splats["means"]
    colors_dc = splats["features_dc"]
    colors_rest = splats["features_rest"]
    colors_all = torch.cat([colors_dc, colors_rest], dim=1)

    colors = colors_dc[:,0,:] #* 0
    colors_0 = colors_dc[:,0,:] * 0
    colmap_project = splats["colmap_project"]


    opacities = torch.sigmoid(splats["opacity"])
    scales = torch.exp(splats["scaling"])
    quats = splats["rotation"]
    K = splats["camera_matrix"]
    colors.requires_grad = True
    colors_0.requires_grad = True

    gaussian_features = torch.zeros(colors.shape[0],DIM, device=colors.device)
    gaussian_denoms = torch.ones(colors.shape[0], device=colors.device) * 1e-12



    t1 = time.time()

    colors_feats = torch.zeros(colors.shape[0],DIM, device=colors.device)
    colors_feats.requires_grad = True
    colors_feats_0 = torch.zeros(colors.shape[0],3, device=colors.device)
    colors_feats_0.requires_grad = True
    cosine_similarity = torch.nn.CosineSimilarity(dim=1)
    print("Distilling features...")
    for image in tqdm(sorted(colmap_project.images.values(), key=lambda x: x.name)):

        image_name = image.name#.split(".")[0] + ".jpg"
        
        viewmat = get_viewmat_from_colmap_image(image)

        
        width = int(K[0, 2] * 2)
        height = int(K[1, 2] * 2)
        with torch.no_grad():
            output, _, meta = rasterization(
                means,
                quats,
                scales,
                opacities,
                colors_all,
                viewmat[None],
                K[None],
                width=width,
                height=height,
                sh_degree=3,
            )

            # output = torch.nn.functional.interpolate(output.permute(0,3,1,2).cuda(), size=(640, 480), mode='bilinear', align_corners=False)
            # feats = net.forward(output)
            # feats = torch.nn.functional.normalize(feats, dim=1)
            # feats = torch.nn.functional.interpolate(feats, size=(height, width), mode='bilinear', align_corners=False)[0]
            # feats = feats.permute(1,2,0)

            # Resize output to 224x224
            output = torch.nn.functional.interpolate(output.permute(0,3,1,2).cuda(), size=(224*4, 224*4), mode='bilinear', align_corners=False)
            feats = dinov2_vits14_reg.forward_features(output)["x_norm_patchtokens"]
            feats = feats[0].reshape((16*4, 16*4, DIM))
            # print(feats.shape)
            # Resize to width x height
            feats = torch.nn.functional.interpolate(feats.unsqueeze(0).permute(0,3,1,2), size=(height, width), mode='nearest')[0]
            # print(feats.shape)
            feats = feats.permute(1,2,0)

        output_for_grad, _, meta = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors_feats,
            viewmat[None],
            K[None],
            width=width,
            height=height,
            # sh_degree=3,
        )


        target = (output_for_grad[0] * feats).mean()

        target.backward()

        colors_feats_copy = colors_feats.grad.clone()

        colors_feats.grad.zero_()

        output_for_grad, _, meta = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors_feats_0,
            viewmat[None],
            K[None],
            width=width,
            height=height,
            # sh_degree=3,
        )

        target_0 = (output_for_grad[0]).mean()

        target_0.backward()
        

        gaussian_features += colors_feats_copy #/ (colors_feats_0.grad[:,0:1]+1e-12)
        gaussian_denoms += colors_feats_0.grad[:,0]
        colors_feats_0.grad.zero_()
        # break
    print(gaussian_features.shape, gaussian_denoms.shape)
    gaussian_features = gaussian_features / gaussian_denoms[...,None]
    gaussian_features = gaussian_features / gaussian_features.norm(dim=-1, keepdim=True)
    # Replace nan values with 0
    print("NaN features", torch.isnan(gaussian_features).sum())
    gaussian_features[torch.isnan(gaussian_features)] = 0
    t2 = time.time()
    print("Time taken for feature distillation", t2-t1)
    return gaussian_features




def get_mask3d_lseg(splats, features, prompt, data_dir, results_dir, threshold=0.9):
    cosine_similarity = torch.nn.CosineSimilarity(dim=1)
    net = LSegNet(
        backbone="clip_vitl16_384",
        features=256,
        crop_size=480,
        arch_option=0,
        block_depth=0,
        activation="lrelu",
    )
    # Load pre-trained weights
    net.load_state_dict(torch.load("./checkpoints/lseg_minimal_e200.ckpt"))
    net.eval()
    net.cuda()

    # Preprocess the text prompt
    clip_text_encoder = net.clip_pretrained.encode_text
    prompt = clip.tokenize([prompt, "other"])
    prompt = prompt.cuda()

    text_feat = clip_text_encoder(prompt)  # 1, 512
    text_feat_norm = torch.nn.functional.normalize(text_feat, dim=1)

    mask_3d = cosine_similarity(features, text_feat_norm[0][None]) > cosine_similarity(features, text_feat_norm[1][None])
    mask_3d = (cosine_similarity(features, text_feat_norm[0][None]) > threshold) & mask_3d
    print("mask_3d.shape", mask_3d.shape)
    mask_3d = mask_3d > threshold
    mask_3d_inv = ~mask_3d

    return mask_3d, mask_3d_inv


def render_pca(splats, features, results_dir, show_visual_feedback):
    
    pca = PCA(n_components=3)
    pca.fit(features.cpu().numpy())
    print("features.shape", features.shape)
    cnt = 0
    colmap_project = splats["colmap_project"]
    means = splats["means"]
    colors_dc = splats["features_dc"]
    colors_rest = splats["features_rest"]
    colors_all = torch.cat([colors_dc, colors_rest], dim=1)
    K = splats["camera_matrix"]
    opacities = torch.sigmoid(splats["opacity"])
    scales = torch.exp(splats["scaling"])
    quats = splats["rotation"]
    # Fit PCA
    pca = PCA(n_components=3)
    pca.fit(features.cpu().numpy())
    colors_pca = pca.transform(features.cpu().numpy())
    colors_pca = colors_pca - colors_pca.min()
    colors_pca = colors_pca / colors_pca.max()
    colors_pca = torch.from_numpy((colors_pca)).float().cuda()
    frames_for_gif = []
    frames_for_gif_frame_pca = []
    for image in sorted(colmap_project.images.values(), key=lambda x: x.name):
        image_name = image.name#.split(".")[0] + ".jpg"
        
        viewmat = get_viewmat_from_colmap_image(image)

        
        width = int(K[0, 2] * 2)
        height = int(K[1, 2] * 2)

        output, _, meta = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors_pca,
            viewmat[None],
            K[None],
            width=width,
            height=height,
            # sh_degree=3,
        )
        # output_for_grad = output_for_grad / output_for_grad.norm(dim=-1, keepdim=True)
        # print(output_for_grad.shape, output_for_grad.min(), output_for_grad.max())
        # output = output[0].detach().cpu().numpy()
        output_cv = torch_to_cv(output[0])

        output, _, meta = rasterization(
            means,
            quats,
            scales,
            opacities,
            features,
            viewmat[None],
            K[None],
            width=width,
            height=height,
            # sh_degree=3,
        )

        
        output_pca = pca.transform(output.detach().cpu().numpy().reshape(-1, DIM)).reshape((height, width, 3))
        # print(output_pca.shape, output_pca.min(), output_pca.max())
        output_pca = output_pca - output_pca.min()
        output_pca = output_pca / output_pca.max()
        output_pca = (output_pca * 255).astype(np.uint8)
        # output_pca = output_pca.reshape((height, width, 3))
        cv2.imshow("PCA by frame", output_pca[...,::-1])
        # cv2.waitKey(1)
        # # break
        # if cnt == 90:
        #     cv2.imwrite("pca.png", output_pca)
        #     break
        # cnt+=1
        cv2.imshow("PCA by scene", output_cv)
        cv2.waitKey(1)
        frames_for_gif_frame_pca.append(output_pca[...,::-1])
        frames_for_gif.append(output_cv)
    imageio.mimsave(f"{results_dir}/pca_dino.gif", frames_for_gif, fps=10, loop=0)
    imageio.mimsave(f"{results_dir}/pca_dino_by_frames.gif", frames_for_gif, fps=10, loop=0)
from torchvision import transforms as T
from gsplat import rasterization

if not torch.cuda.is_available():
    raise RuntimeError("CUDA is required for this script")
torch.set_default_device("cuda")
device = "cuda"

SIZE = 224 * 4
FEATURE_MAP_SIZE = 64

LABEL_TO_IDX = {
    "background": 0,
    "grasp": 1,
    "cut": 2,
    "scoop": 3,
    "contain": 4,
    "pound": 5,
    "support": 6,
    "wrap grasp": 7,
}

IDX_TO_LABEL = ["background", "grasp", "cut", "scoop", "contain", "pound", "support", "wrap grasp"]

transform = T.Compose([T.Resize((SIZE, SIZE)), T.ToTensor(), T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
PATCH_SIZE = 14

DIM = 1024
from PIL import Image

LABEL_TO_IDX = {
    "background": 0,
    "grasp": 1,
    "cut": 2,
    "scoop": 3,
    "contain": 4,
    "pound": 5,
    "support": 6,
    "wrap grasp": 7,
}

IDX_TO_LABEL = ["background", "grasp", "cut", "scoop", "contain", "pound", "support", "wrap grasp"]



feature_extractor = (
    torch.hub.load("facebookresearch/dinov2:main", "dinov2_vitl14_reg")
    .to(device)
    .eval()
)


def load_labels(labels_dir, results_dir):
    example_images = os.listdir(labels_dir)
    example_images = [img for img in example_images if img.endswith(".webp")]
    features = []
    labels = []
    for example in example_images:
        example_path = os.path.join(labels_dir, example)
        annotations = json.load(open(example_path.replace(".webp", ".json")))

        img_example = cv2.imread(example_path)[..., ::-1]
        img_example = Image.fromarray(img_example)
        img_example_th = transform(img_example).to(device)
        feats = feature_extractor.forward_features(img_example_th[None])[
            "x_norm_patchtokens"
        ][0]
        feats = feats.reshape((FEATURE_MAP_SIZE, FEATURE_MAP_SIZE, -1))
        feats = feats / torch.norm(feats, dim=-1, keepdim=True)
        feats_flatten = feats.detach().cpu().numpy().reshape((-1, DIM))

        background_mask_inverted = np.zeros((FEATURE_MAP_SIZE, FEATURE_MAP_SIZE))

        for shape in annotations["shapes"]:
            shape_label = shape["label"]
            assert shape_label in LABEL_TO_IDX
            label_idx = LABEL_TO_IDX[shape_label]
            mask64 = shape["mask"]
            mask_bytes = base64.b64decode(mask64)
            mask = (
                cv2.imdecode(np.frombuffer(mask_bytes, np.uint8), cv2.IMREAD_UNCHANGED)
                * 255
            )
            boundary_points = np.array(shape["points"]).astype(np.int32)
            blank_image = np.zeros_like(img_example)
            blank_image[
                boundary_points[0][1] : boundary_points[1][1] + 1,
                boundary_points[0][0] : boundary_points[1][0] + 1,
            ] = mask[..., None]
            blank_image = blank_image[..., 0]
            mask_np = cv2.resize(
                blank_image,
                (FEATURE_MAP_SIZE, FEATURE_MAP_SIZE),
                interpolation=cv2.INTER_NEAREST,
            )
            mask_flatten = mask_np.flatten()
            feats_current_mask = feats_flatten[mask_flatten > 0]
            labels_current_mask = np.ones((feats_current_mask.shape[0], 1)) * label_idx

            features = (
                np.concatenate([features, feats_current_mask], axis=0)
                if len(features) > 0
                else feats_current_mask
            )
            labels = (
                np.concatenate([labels, labels_current_mask], axis=0)
                if len(labels) > 0
                else labels_current_mask
            )
            background_mask_inverted = np.logical_or(background_mask_inverted, mask_np)
            cv2.imshow("mask", mask_np)
            cv2.waitKey(1)
            cv2.destroyAllWindows()

        background_mask = ~background_mask_inverted
        features_background = feats_flatten[background_mask.flatten()]
        labels_background = np.zeros((features_background.shape[0], 1))
        features = (
            np.concatenate([features, features_background], axis=0)
            if len(features) > 0
            else features_background
        )
        labels = (
            np.concatenate([labels, labels_background], axis=0)
            if len(labels) > 0
            else labels_background
        )
        cv2.imshow("background_mask", background_mask.astype(np.uint8) * 255)
        cv2.waitKey(1)
        cv2.destroyAllWindows()
        data = {"features": features, "labels": labels}
        os.makedirs(results_dir, exist_ok=True)
        pkl.dump(data, open(os.path.join(results_dir, "features_and_labels.pkl"), "wb"))

def transfer_affordance(splats, features, results_dir):
    def most_frequent(array):
        return np.bincount(array).argmax()
    # images_dir = os.path.join(results_dir, "images")
    # affordance_map_dir = os.path.join(results_dir, "affordance_maps")
    # affordance_map_images_dir = os.path.join(results_dir, "affordance_map_images")
    # os.makedirs(affordance_map_images_dir, exist_ok=True)
    # os.makedirs(affordance_map_dir, exist_ok=True)
    features_and_labels = pkl.load(open(os.path.join(results_dir, "features_and_labels.pkl"), "rb"))

    features_source = features_and_labels["features"]
    labels = features_and_labels["labels"]
    feature_index = faiss.IndexFlatIP(DIM)
    feature_index.add(features_source)
    features_dest = features.cpu().numpy()
    D, I = feature_index.search(features_dest, 5)
    label_map = labels[I].astype(np.int32)
    result = np.apply_along_axis(most_frequent, axis=1, arr=label_map)
    # print(result.shape)
    return result


def render_affordance(splats, results, results_dir):
    means = splats["means"]
    colors_dc = splats["features_dc"]
    colors_rest = splats["features_rest"]
    colors_all = torch.cat([colors_dc, colors_rest], dim=1)
    K = splats["camera_matrix"]
    opacities = torch.sigmoid(splats["opacity"])
    scales = torch.exp(splats["scaling"])
    quats = splats["rotation"]
    colmap_project = splats["colmap_project"]

    colors_new = colors_all.clone()
    color_palette = np.array([[125, 125, 125], [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0], [255, 0, 255], [0, 255, 255], [128, 0, 0]])

    for i in range(1,8):
        mask = results[:,0] == i
        colors_new[mask, 0, :3] = colors_new[mask, 0, :3] * 0.5 + 0.5*((torch.from_numpy(color_palette[i])/255.).to(device)-0.5)/ (1/np.sqrt(4*np.pi))
        colors_new[:, 1:, :3] = 0.0 * colors_new[:, 1:, :3]

    affordance_gif_frames = []
    writer = cv2.VideoWriter(os.path.join(results_dir, "affordance_transfer.mp4"), cv2.VideoWriter_fourcc(*'mp4v'), 20, (int(K[0,2]*2), int(K[1,2]*2)))
    for cnt, image in enumerate(natsorted(list(colmap_project.images.values()), key=lambda x: x.name)):
        viewmat = get_viewmat_from_colmap_image(image)
        alpha_factor = min(1.0, cnt/20)
        output, _, _ = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors_new*alpha_factor + colors_all*(1-alpha_factor),
            viewmats=viewmat[None],
            Ks=K[None],
            sh_degree=3,
            width=K[0, 2] * 2,
            height=K[1, 2] * 2,
            backgrounds=torch.ones((3,)).cuda()
        )
        output_cv = torch_to_cv(output[0])
        cv2.imshow("Output", output_cv)
        cv2.waitKey(1)
        affordance_gif_frames.append(output_cv[...,::-1])
        writer.write(output_cv)
    cv2.imwrite(os.path.join(results_dir, "affordance_transfer.png"),affordance_gif_frames[0][...,::-1])
    imageio.mimsave(f"{results_dir}/affordance_transfer.gif", affordance_gif_frames, fps=20, loop=0)


from scipy.io import loadmat
from collections import defaultdict

def evaluate_results(data_dir, splats, results, results_dir):
    gt_dir = os.path.join(data_dir, "gt")
    # affordance_maps_dir = os.path.join(results_dir, "affordance_maps")
    
    # Not rendering now
    # splats = load_checkpoint(checkpoint, data_dir, rasterizer="gsplat")
    colmap_project = splats["colmap_project"]
    label_files = os.listdir(gt_dir)
    label_files = [label_file for label_file in label_files if label_file.endswith("label.mat")]
    label_files.sort()
    mIoU = defaultdict(list)
    recall = defaultdict(list)
    assert len(label_files) == len(colmap_project.images)

    # For affordance map
    if False:
        print("\n\nEvaluating 2D-2D affordance transfer.")
        for image, label_file in zip(sorted(colmap_project.images.values(), key=lambda x: x.name), label_files):
            # gt_name = image.name.replace("_rgb.jpg", "_label.mat")
            # image_path = os.path.join(data_dir, "images", image.name)
            # gt_image_path = os.path.join(gt_dir, label_file.replace("_label.mat", "_rgb.jpg"))
            gt_path = os.path.join(gt_dir, label_file)
            gt = loadmat(gt_path)
            if gt["gt_type"] == "automatic":
                continue
            gt_label = gt["gt_label"]

            affordance_map_name = image.name.replace(".jpg", ".npy")
            affordance_map_path = os.path.join(affordance_maps_dir, affordance_map_name)
            affordance_map = np.load(affordance_map_path)
            affordance_map = cv2.resize(affordance_map, (gt_label.shape[1], gt_label.shape[0]), interpolation=cv2.INTER_NEAREST)

            for i in range(1, 8):
                gt_mask = gt_label == i
                affordance_mask = affordance_map == i
                intersection = np.logical_and(gt_mask, affordance_mask).sum()
                union = np.logical_or(gt_mask, affordance_mask).sum()
                if union == 0:
                    continue
                if intersection == 0:
                    iou = 0
                else:
                    iou = intersection / union
                mIoU[i].append(iou)

                if gt_mask.sum() == 0:
                    continue
                if intersection == 0:
                    rec = 0
                else:
                    rec = intersection / gt_mask.sum() # Actually recall
                recall[i].append(rec)
                # print(f"Class {i}: {iou}")
            # Calculate mIoU
        res = 0
        for i in range(1, 8):
            if len(mIoU[i]) == 0:
                continue
            res += np.mean(mIoU[i])
            # print(f"mIoU {i}: {np.mean(mIoU[i])}")
        res /= (np.unique(gt_label).size - 1)
        print(f"mIoU: {res}")

        res_recall = 0
        for i in range(1, 8):
            if len(recall[i]) == 0:
                continue
            res_recall += np.mean(recall[i])
            # print(f"Recall {i}: {np.mean(recall[i])}")
        res_recall /= (np.unique(gt_label).size - 1)
        print(f"Recall: {res_recall}")


    print("\nEvaluating 2D-3D affordance transfer.")
    mIoU = defaultdict(list)
    recall = defaultdict(list)
    means = splats["means"]
    colors_dc = splats["features_dc"]
    colors_rest = splats["features_rest"]
    colors = torch.cat([colors_dc, colors_rest], dim=1)
    opacities = torch.sigmoid(splats["opacity"])
    scales = torch.exp(splats["scaling"])
    quats = splats["rotation"]
    K = splats["camera_matrix"]
    width = int(K[0, 2] * 2)
    height = int(K[1, 2] * 2)

    # votes_path = os.path.join(results_dir, "votes.npy")
    # votes = np.load(votes_path)
    votes = results
    votes = torch.from_numpy(votes).to(device)
    for image, label_file in zip(sorted(colmap_project.images.values(), key=lambda x: x.name), label_files):
        gt_name = image.name.replace("_rgb.jpg", "_label.mat")
        image_path = os.path.join(data_dir, "images", image.name)
        gt_image_path = os.path.join(gt_dir, label_file.replace("_label.mat", "_rgb.jpg"))

        gt_path = os.path.join(gt_dir, label_file)
        gt = loadmat(gt_path)
        if gt["gt_type"] == "automatic":
            continue
        gt_label = gt["gt_label"]

        # affordance_map_name = image.name.replace(".jpg", ".npy")
        # affordance_map_path = os.path.join(affordance_maps_dir, affordance_map_name)
        # affordance_map = np.load(affordance_map_path)
        # affordance_map = cv2.resize(affordance_map, (gt_label.shape[1], gt_label.shape[0]), interpolation=cv2.INTER_NEAREST)

        viewmat = get_viewmat_from_colmap_image(image)


        for i in range(1, 8):
            colors_new = colors.clone()[:,0]
            mask = votes[:,0] == i
            colors_new[mask] = 1.0
            colors_new[~mask] = 0.0

            output, _, meta = rasterization(
                means,
                quats,
                scales,
                opacities,
                colors_new,
                viewmat[None],
                K[None],
                width=width,
                height=height,
                # backgrounds=torch.tensor([[0.0, 0.0, 0.0]]).to(device),
                backgrounds=torch.zeros((3,)).to(device),
            )
            output_cv = torch_to_cv(output[0, ..., :3].detach())
            cv2.imshow("Mapped affordance regions", output_cv)
            cv2.waitKey(1)
            gt_mask = gt_label == i
            affordance_mask = output_cv > 64
            affordance_mask = affordance_mask[...,0]
            intersection = np.logical_and(gt_mask, affordance_mask).sum()
            union = np.logical_or(gt_mask, affordance_mask).sum()
            if union == 0:
                continue
            if intersection == 0:
                iou = 0
            else:
                iou = intersection / union
            mIoU[i].append(iou)
            if gt_mask.sum() == 0:
                continue
            if intersection == 0:
                rec = 0
            else:
                rec = intersection / gt_mask.sum()
            recall[i].append(rec)
    res = 0
    for i in range(1, 8):
        if len(mIoU[i]) == 0:
            continue
        res += np.mean(mIoU[i])
        # print(f"mIoU {i}: {np.mean(mIoU[i])}")
    res /= (np.unique(gt_label).size - 1)
    print(f"mIoU: {res}")

    res_recall = 0
    for i in range(1, 8):
        if len(recall[i]) == 0:
            continue
        res_recall += np.mean(recall[i])
        # print(f"Recall {i}: {np.mean(recall[i])}")
    res_recall /= (np.unique(gt_label).size - 1)
    print(f"Recall: {res_recall}")
    

def main(
        data_dir: str = "./data/chair", # colmap path
        checkpoint: str = "./data/chair/checkpoint.pth", # checkpoint path, can generate from original 3DGS repo
        prompt: str = "chair", # prompt
        results_dir: str = "./results/chair", # output path
        show_visual_feedback: bool = True, # Will show opencv window,
        rasterizer: Literal["original", "gsplat"] = "original", # Original or GSplat for checkpoints
        data_factor: int = 1,
        label_dir: str = "./data/affordance_labels",
        threshold: float = 0.9,
):
    
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA is required for this demo")
    
    torch.set_default_device('cuda')

    load_labels(label_dir, results_dir)

    os.makedirs(results_dir, exist_ok=True)
    splats = load_checkpoint(checkpoint, data_dir, rasterizer=rasterizer, data_factor=data_factor)
    splats_optimized = prune_by_gradients(splats)
    test_proper_pruning(splats, splats_optimized)

    del splats
    splats = splats_optimized

    # render_to_dir(f"{results_dir}/images", splats, show_visual_feedback)
    # create_feature_field_dino(splats, results_dir)
    # features = create_feature_field_dino(splats, results_dir)
    # torch.save(features, f"{results_dir}/features_dino.pt")
    features = torch.load(f"{results_dir}/features_dino_garden.pt")
    # mask3d, mask3d_inverted = get_mask3d_lseg(splats, features, prompt, data_dir, results_dir, threshold=threshold)
    # print(mask3d.shape, mask3d_inverted.shape, mask3d.sum(), mask3d_inverted.sum())
    # create_feature_field_maskclip(splats, results_dir)
    # mask3d, mask3d_inverted = get_mask3d(splats, prompt, data_dir, results_dir, show_visual_feedback, mask_interval=mask_interval, voting_method=voting_method)

    # extracted, deleted, masked = apply_mask3d(splats, mask3d, mask3d_inverted, results_dir)

    # render_pca(splats, features, results_dir, show_visual_feedback)
    t1 = time.time()
    results = transfer_affordance(splats, features, results_dir)
    t2 = time.time()
    print("Time taken for affordance transfer", t2-t1)
    render_affordance(splats,results, results_dir)
    exit()
    evaluate_results(data_dir, splats, results, results_dir)
    # # render_mask_pred(f"{results_dir}/mask_bin_pred", masked, show_visual_feedback)
    # render_to_gif(f"{results_dir}/extracted.gif", extracted, show_visual_feedback, use_checkerboard_background=True)
    # render_to_gif(f"{results_dir}/deleted.gif", deleted, show_visual_feedback)
    # render_to_gif(f"{results_dir}/masked.gif", masked, show_visual_feedback, no_sh=True)


if __name__ == "__main__":
    tyro.cli(main)