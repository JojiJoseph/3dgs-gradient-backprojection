# Basic OpenCV viewer with sliders for rotation and translation.
# Can be easily customizable to different use cases.
import torch
from gsplat import rasterization
import cv2
import tyro
import numpy as np
import json
from typing import Literal
import pycolmap_scene_manager as pycolmap
import clip
from lseg import LSegNet

device = torch.device("cuda:0")

def get_rpy_matrix(roll, pitch, yaw):
    roll_matrix = np.array(
        [
            [1, 0, 0, 0],
            [0, np.cos(roll), -np.sin(roll), 0],
            [0, np.sin(roll), np.cos(roll), 0],
            [0, 0, 0, 1.0],
        ])
    
    pitch_matrix = np.array(
        [
            [np.cos(pitch), 0, np.sin(pitch), 0],
            [0, 1, 0, 0],
            [-np.sin(pitch), 0, np.cos(pitch), 0],
            [0, 0, 0, 1.0],
        ])
    yaw_matrix = np.array(
        [
            [np.cos(yaw), -np.sin(yaw), 0, 0],
            [np.sin(yaw), np.cos(yaw), 0, 0],
            [0, 0, 1, 0],
            [0, 0, 0, 1.0],
        ]

    )

    return yaw_matrix @ pitch_matrix @ roll_matrix



def _detach_tensors_from_dict(d, inplace=True):
    if not inplace:
        d = d.copy()
    for key in d:
        if isinstance(d[key], torch.Tensor):
            d[key] = d[key].detach()
    return d


def load_checkpoint(checkpoint: str, data_dir: str, rasterizer: Literal["inria", "gsplat"]="gsplat", data_factor: int = 1):

    colmap_project = pycolmap.SceneManager(f"{data_dir}/sparse/0")
    colmap_project.load_cameras()
    colmap_project.load_images()
    colmap_project.load_points3D()
    model = torch.load(checkpoint) # Make sure it is generated by 3DGS original repo
    if rasterizer == "original":
        model_params, _ = model
        splats = {
            "active_sh_degree": model_params[0],
            "means": model_params[1],
            "features_dc": model_params[2],
            "features_rest": model_params[3],
            "scaling": model_params[4],
            "rotation": model_params[5],
            "opacity": model_params[6].squeeze(1),
        }
    elif rasterizer == "gsplat":

        model_params = model["splats"]
        splats = {
            "active_sh_degree": 3,
            "means": model_params["means"],
            "features_dc": model_params["sh0"],
            "features_rest": model_params["shN"],
            "scaling": model_params["scales"],
            "rotation": model_params["quats"],
            "opacity": model_params["opacities"],
        }
    else:
        raise ValueError("Invalid rasterizer")

    _detach_tensors_from_dict(splats)

    # Assuming only one camera
    for camera in colmap_project.cameras.values():
        camera_matrix = torch.tensor(
            [
                [camera.fx, 0, camera.cx],
                [0, camera.fy, camera.cy],
                [0, 0, 1],
            ]
        )
        break

    camera_matrix[:2,:3] /= data_factor

    splats["camera_matrix"] = camera_matrix
    splats["colmap_project"] = colmap_project
    splats["colmap_dir"] = data_dir

    return splats

def get_viewmat_from_colmap_image(image):
    viewmat = torch.eye(4).float()  # .to(device)
    viewmat[:3, :3] = torch.tensor(image.R()).float()  # .to(device)
    viewmat[:3, 3] = torch.tensor(image.t).float()  # .to(device)
    return viewmat

def prune_by_gradients(splats):
    colmap_project = splats["colmap_project"]
    frame_idx = 0
    means = splats["means"]
    colors_dc = splats["features_dc"]
    colors_rest = splats["features_rest"]
    colors = torch.cat([colors_dc, colors_rest], dim=1)
    opacities = torch.sigmoid(splats["opacity"])
    scales = torch.exp(splats["scaling"])
    quats = splats["rotation"]
    K = splats["camera_matrix"]
    colors.requires_grad = True
    gaussian_grads = torch.zeros(colors.shape[0], device=colors.device)
    for image in sorted(colmap_project.images.values(), key=lambda x: x.name):
        viewmat = get_viewmat_from_colmap_image(image)
        output, _, _ = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors[:, 0, :],
            viewmats=viewmat[None],
            Ks=K[None],
            # sh_degree=3,
            width=K[0, 2] * 2,
            height=K[1, 2] * 2,
        )
        frame_idx += 1
        pseudo_loss = ((output.detach() + 1 - output) ** 2).mean()
        pseudo_loss.backward()
        # print(colors.grad.shape)
        gaussian_grads += (colors.grad[:, 0]).norm(dim=[1])
        colors.grad.zero_()

    mask = gaussian_grads > 0
    print("Total splats", len(gaussian_grads))
    print("Pruned", (~mask).sum(), "splats")
    print("Remaining", mask.sum(), "splats")
    splats = splats.copy()
    splats["means"] = splats["means"][mask]
    splats["features_dc"] = splats["features_dc"][mask]
    splats["features_rest"] = splats["features_rest"][mask]
    splats["scaling"] = splats["scaling"][mask]
    splats["rotation"] = splats["rotation"][mask]
    splats["opacity"] = splats["opacity"][mask]
    return splats

def create_checkerboard(width, height, size=64):
    checkerboard = np.zeros((height, width, 3), dtype=np.uint8)
    for y in range(0, height, size):
        for x in range(0, width, size):
            if (x // size + y // size) % 2 == 0:
                checkerboard[y:y + size, x:x + size] = 255
            else:
                checkerboard[y:y + size, x:x + size] = 128
    return checkerboard


def main(data_dir: str = "./data/garden", # colmap path
        checkpoint: str = "./data/garden/ckpts/ckpt_29999_rank0.pt", # checkpoint path, can generate from original 3DGS repo
        rasterizer: Literal["inria", "gsplat"] = "gsplat", # Original or GSplat for checkpoints
        results_dir: str = "./results/garden",
        data_factor: int = 4):
    
    torch.set_default_device("cuda")

    splats = load_checkpoint(checkpoint, data_dir, rasterizer=rasterizer, data_factor=data_factor)
    splats = prune_by_gradients(splats)
    torch.set_grad_enabled(False)

    means = splats["means"].float()
    opacities = splats["opacity"]
    quats = splats["rotation"]
    scales = splats["scaling"].float()

    opacities = torch.sigmoid(opacities)
    scales = torch.exp(scales)
    colors = torch.cat([splats["features_dc"], splats["features_rest"]], 1)
    features = torch.load(f"{results_dir}/features_lseg.pt")

    cv2.namedWindow("Click and Segment", cv2.WINDOW_NORMAL)
    cv2.createTrackbar("Roll", "Click and Segment", 0, 180, lambda x: None)
    cv2.createTrackbar("Pitch", "Click and Segment", 0, 180, lambda x: None)
    cv2.createTrackbar("Yaw", "Click and Segment", 0, 180, lambda x: None)
    cv2.createTrackbar("X", "Click and Segment", 0, 1000, lambda x: None)
    cv2.createTrackbar("Y", "Click and Segment", 0, 1000, lambda x: None)
    cv2.createTrackbar("Z", "Click and Segment", 0, 1000, lambda x: None)
    cv2.createTrackbar("Scaling", "Click and Segment", 100, 100, lambda x: None)

    cv2.setTrackbarMin("Roll", "Click and Segment", -180)
    cv2.setTrackbarMax("Roll", "Click and Segment", 180)
    cv2.setTrackbarMin("Pitch", "Click and Segment", -180)
    cv2.setTrackbarMax("Pitch", "Click and Segment", 180)
    cv2.setTrackbarMin("Yaw", "Click and Segment", -180)
    cv2.setTrackbarMax("Yaw", "Click and Segment", 180)
    cv2.setTrackbarMin("X", "Click and Segment", -1000)
    cv2.setTrackbarMax("X", "Click and Segment", 1000)
    cv2.setTrackbarMin("Y", "Click and Segment", -1000)
    cv2.setTrackbarMax("Y", "Click and Segment", 1000)
    cv2.setTrackbarMin("Z", "Click and Segment", -1000)
    cv2.setTrackbarMax("Z", "Click and Segment", 1000)


    K = splats["camera_matrix"].float()


    width = int(K[0, 2] * 2)
    height = int(K[1, 2] * 2)

    positive_prompt_locations = []
    negative_prompt_locations = []

    

    net = LSegNet(
        backbone="clip_vitl16_384",
        features=256,
        crop_size=480,
        arch_option=0,
        block_depth=0,
        activation="lrelu",
    )
    # Load pre-trained weights
    net.load_state_dict(torch.load("./checkpoints/lseg_minimal_e200.ckpt"))
    net.eval()
    net.cuda()

    # Preprocess the text prompt
    clip_text_encoder = net.clip_pretrained.encode_text
    
    other_prompt = clip.tokenize(["other"])
    other_prompt = other_prompt.cuda()
    other_prompt = clip_text_encoder(other_prompt)  # N, 512, N - number of prompts
    other_prompt = torch.nn.functional.normalize(other_prompt, dim=1).float()

    mask_3d = None

    def callback(event, x, y, flags, param):
        trigger = False
        ctrl_pressed = flags & cv2.EVENT_FLAG_CTRLKEY
        if event == cv2.EVENT_LBUTTONDOWN:
            # check if ctrl is pressed
            if ctrl_pressed:
                del_idx = None
                for i, (x_i, y_i) in enumerate(positive_prompt_locations):
                    if abs(x_i - x) < 10 and abs(y_i - y) < 10:
                        del_idx = i
                        break
                if del_idx is not None:
                    del positive_prompt_locations[del_idx]
            else:
                positive_prompt_locations.append((x, y))
            trigger = True
        elif event == cv2.EVENT_MBUTTONDOWN:
            if ctrl_pressed:
                del_idx = None
                for i, (x_i, y_i) in enumerate(negative_prompt_locations):
                    if abs(x_i - x) < 40 and abs(y_i - y) < 40:
                        del_idx = i
                        break
                if del_idx is not None:
                    del negative_prompt_locations[del_idx]
            else:
                negative_prompt_locations.append((x, y))
            trigger = True
        if trigger:
            roll = cv2.getTrackbarPos("Roll", "Click and Segment")
            pitch = cv2.getTrackbarPos("Pitch", "Click and Segment")
            yaw = cv2.getTrackbarPos("Yaw", "Click and Segment")

            roll_rad = np.deg2rad(roll)
            pitch_rad = np.deg2rad(pitch)
            yaw_rad = np.deg2rad(yaw)

            viewmat = (
                torch.tensor(get_rpy_matrix(roll_rad, pitch_rad, yaw_rad))
                .float()
                .to(device)
            )
            viewmat[0, 3] = cv2.getTrackbarPos("X", "Click and Segment") / 100.0
            viewmat[1, 3] = cv2.getTrackbarPos("Y", "Click and Segment") / 100.0
            viewmat[2, 3] = cv2.getTrackbarPos("Z", "Click and Segment") / 100.0
            output, alphas, meta = rasterization(
                means,
                quats,
                scales * cv2.getTrackbarPos("Scaling", "Click and Segment") / 100.0,
                opacities,
                features,
                viewmat[None],
                K[None],
                width=width,
                height=height,
                # sh_degree=3,
            )

            output = output[0]
            output = torch.nn.functional.normalize(output, dim=-1)

            positive_prompts = []
            negative_prompts = [other_prompt[0]]
            if x < width:
                for x, y in positive_prompt_locations:
                    positive_prompts.append(output[y, x])
                for x, y in negative_prompt_locations:
                    negative_prompts.append(output[y, x])
                nonlocal mask_3d
                if not positive_prompt_locations:
                    mask_3d = None
                else:
                    positive_prompts = torch.stack(positive_prompts) # [P, 512]
                    negative_prompts = torch.stack(negative_prompts) # [P, 512]

                    scores_pos = features @ positive_prompts.T # [N, P]
                    scores_pos = scores_pos.max(dim=1) # [N]
                    scores_neg = features @ negative_prompts.T # [N, P]
                    scores_neg = scores_neg.max(dim=1) # [N]
                    mask_3d = scores_pos.values > scores_neg.values


    cv2.setMouseCallback("Click and Segment", callback)

    while True:
        for image in splats["colmap_project"].images.values():
            viewmat_cmap = get_viewmat_from_colmap_image(image)
            roll = cv2.getTrackbarPos("Roll", "Click and Segment")
            pitch = cv2.getTrackbarPos("Pitch", "Click and Segment")
            yaw = cv2.getTrackbarPos("Yaw", "Click and Segment")

            roll_rad = np.deg2rad(roll)
            pitch_rad = np.deg2rad(pitch)
            yaw_rad = np.deg2rad(yaw)

            viewmat = (
                torch.tensor(get_rpy_matrix(roll_rad, pitch_rad, yaw_rad))
                .float()
                .to(device)
            )
            viewmat[0, 3] = cv2.getTrackbarPos("X", "Click and Segment") / 100.0
            viewmat[1, 3] = cv2.getTrackbarPos("Y", "Click and Segment") / 100.0
            viewmat[2, 3] = cv2.getTrackbarPos("Z", "Click and Segment") / 100.0
            output, alphas, meta = rasterization(
                means,
                quats,
                scales * cv2.getTrackbarPos("Scaling", "Click and Segment") / 100.0,
                opacities,
                colors,
                viewmat[None],
                K[None],
                width=width,
                height=height,
                sh_degree=3,
            )

            output_cv = torch_to_cv(output[0])

            if mask_3d is not None:
                opacities_new = opacities.clone()
                opacities_new2 = opacities.clone()
                opacities_new[~mask_3d] = 0
                opacities_new2[mask_3d] = 0
            else:
                opacities_new = opacities
                opacities_new2 = opacities
            output, alphas, meta = rasterization(
                means,
                quats,
                scales * cv2.getTrackbarPos("Scaling", "Click and Segment") / 100.0,
                opacities_new,
                colors,
                viewmat_cmap[None],
                K[None],
                width=width,
                height=height,
                sh_degree=3,
            )
            output_cv2 = torch_to_cv(output[0])
            output, alphas, meta = rasterization(
                means,
                quats,
                scales * cv2.getTrackbarPos("Scaling", "Click and Segment") / 100.0,
                opacities_new2,
                colors,
                viewmat_cmap[None],
                K[None],
                width=width,
                height=height,
                sh_degree=3,
            )
            output_cv3 = torch_to_cv(output[0])
            output_cv = cv2.hconcat([output_cv, output_cv2, output_cv3])
            for x,y in positive_prompt_locations:
                cv2.circle(output_cv, (x, y), 20, (0, 255, 0), -1)
            for x,y in negative_prompt_locations:
                cv2.circle(output_cv, (x, y), 20, (0, 0, 255), -1)
            cv2.imshow("Click and Segment", output_cv)
            key = cv2.waitKey(10)
            if key == ord("q"):
                break
        if key == ord("q"):
            break


def torch_to_cv(tensor, permute=False):
    if permute:
        tensor = torch.clamp(tensor.permute(1, 2, 0), 0, 1).cpu().numpy()
    else:
        tensor = torch.clamp(tensor, 0, 1).cpu().numpy()
    return (tensor * 255).astype(np.uint8)[..., ::-1]


if __name__ == "__main__":
    tyro.cli(main)
